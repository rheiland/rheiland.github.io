<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!--Converted with LaTeX2HTML 98.1p1 release (March 2nd, 1998)
originally by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Heiland: Thesis (KLTool)</TITLE>
<META NAME="description" CONTENT="Thesis">
<META NAME="keywords" CONTENT="thesis">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<LINK REL="STYLESHEET" HREF="thesis.css">
<LINK REL="next" HREF="node1.html">
</HEAD>
<BODY BGCOLOR="#FFFFFF">
<!--Navigation Panel-->
<center>
<p>
<h3>KLTOOL: A Mathematical Tool for Analyzing Spatiotemporal Data</h3>
Randy W. Heiland<br>
Arizona State University, Dept of Mathematics, Dec. 1992
<p>
</center>
<P>
<a href="thesis.html#chpt1">1. Introduction</a><br>
<a href="thesis.html#chpt2">2. Spatiotemporal Data</a><br>
<a href="thesis.html#chpt3">3. Dynamical Systems Concepts</a><br>
<a href="thesis.html#chpt4">4. Karhunen-Lo&#232;ve Decomposition</a><br>
<a href="thesis.html#chpt5">5. Overview of <EM>kltool</EM></a><br>
<a href="thesis.html#chpt6">6. Examples</a><br>
<a href="thesis.html#chpt7">7. Future Directions</a><br>
<a href="thesis.html#chpt8">8. Summary</a><br>
<a href="thesis.html#biblio"> Bibliography</a><br>
<a href="thesis.html#appdx"> Appendix: Gal&#235;rkin Projection for Kuramoto-Sivashinsky PDE</a>
<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt1">
	 <B>1. Introduction</B>
</a>
<P>
The quantitative analysis of low-dimensional chaotic dynamical systems
has been an active area of research for many years.
Up until now, most work has concentrated on the analysis of time series
data from laboratory experiments and numerical simulations.
Examples include Rayleigh-B&#233;nard convection, Couette-Taylor fluid flow,
and the Belousov-Zhabotinskii chemical reaction 
[Libchaber, Fauve &amp; Laroche '83], [Roux '83] and [Swinney '84].

<P>
The key idea is to reconstruct a representation of the underlying
attractor from the time series.  (The time-delay embedding method [Takens '81]
is one popular approach).  Given the reconstructed attractor, it is
possible to estimate various properties of the dynamics - Lyapunov exponents
and an attractor's dimension, for example.
Local approximations of the dynamics make it possible to reduce noise
in the original time series and to make short-term predictions of the
system's behavior.
[Kostelich &amp; Yorke '90] and [Sugihara &amp; May '90].

<P>
In contrast, methods for the analysis of spatiotemporal data are not
as well developed.
This thesis describes a computer-aided tool, called <EM>kltool</EM>,
which operates on spatiotemporal data from nonlinear systems.
Its primary function is to analyze the data and extract
a set of basic building blocks - the eigenfunctions of the dataset.  
These eigenfunctions are optimal, in a certain least-squares sense, and
they can reveal coherent spatial structures within the data.
It is then possible to reconstruct an approximation to the data using
these eigenfunctions.  In addition to providing these analytic and 
synthetic modes
of operation, <EM>kltool</EM> has other capabilities which offer insight into
dynamical models for the data.

<P>
Although the underlying mathematical analysis is well known, 
there has not been an interactive software package to apply it to
a given dataset.
<EM>Kltool</EM> is intended to be an easy-to-use, general-purpose 
software package for nonspecialists.
In addition to describing this software, 
we present some new and interesting results obtained
during the tool's development.

<P>
In the next Chapter, we discuss further
the spatiotemporal data to be analyzed.
In Chapter 3, dynamical systems and related concepts are introduced.
Chapter 4 describes the mathematical details of the Karhunen-Lo&#232;ve (K-L)
decomposition - the analytic tool to be applied to a given dataset.
<EM>Kltool</EM>, which incorporates the K-L algorithm with accompanying software, 
will be explained in Chapter 5.  We present some results of
<EM>kltool's</EM> analysis of different datasets in Chapter 6.
Chapter 7 offers a few thoughts on future directions and we
summarize in Chapter 8.

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt2">
	 <B>2. Spatiotemporal Data</B>
</a>
<P>
The type of data we wish to analyze are generated by some nonlinear system 
which evolves in time and which is spatially inhomogeneous.
We assume a spatial vector, <IMG
 WIDTH="17" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="${\bf X}$">,
is obtained
at discrete time values, <I>t</I><SUB><I>i</I></SUB>, providing a set of spatiotemporal data:

<!-- MATH: $\{{\bf X}_i\}_{i=1}^M$ -->
<IMG
 WIDTH="60" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img2.gif"
 ALT="$\{{\bf X}_i\}_{i=1}^M$">.
The components of <IMG
 WIDTH="17" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img1.gif"
 ALT="${\bf X}$">
correspond to scalar measurements taken
at fixed points in space.
We allow for the visualization of one- or two-dimensional spatial vectors:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="218" HEIGHT="141"
 SRC="img3.gif">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
Two common methods of obtaining such spatiotemporal data are from 
laboratory experiments and numerical simulations of a mathematical model -
typically a partial differential equation (PDE).

<P>
The problem we address is: can this complex set of data be understood and
explained in some relatively simple manner?  
It would seem to be possible if the complexity is generated by the
temporal evolution of only a few spatial structures.

<P>
A standard approach in analyzing such data is to perform a Fourier
decomposition.  One would hope that only a few dominant peaks appear
in a power spectrum of the spatial modes, suggesting relatively simple 
spatial structures.
However, it may be that a coherent spatial structure is 
composed of many Fourier modes
and a Fourier analysis would not be the best approach.

<P>
The decomposition used in this thesis, and discussed further in Chapter 4, can
compute coherent spatial structures directly.
Furthermore, the structures will be optimal (in a least-squares sense)
for a given dataset.
Once the primary spatial structures are known, we will try to
explain their temporal evolution within the framework of
dynamical systems theory.  The key concepts of this theory will be presented
in the next chapter.  However, as a prelude, we offer the following thought. 

<P>
A primary objective in applying dynamical systems theory to a nonlinear
system is to understand the long-term (asymptotic) behavior of the system
in a qualitative way.  If the system is sufficiently complex, it is unlikely
that one can make any quantitative predictions about the system.
However, if it is possible to reduce the number of 
degrees of freedom inherent in
the system then certain qualitative descriptions may be possible. 

<P>
Consider a PDE as a nonlinear system.  One could think of such a system
as having ultimate complexity -  by virtue of having infinitely many
degrees of freedom.  It is known that the solution of many PDEs can be
expanded in an infinite set of Fourier modes [Strang '86].
However, for certain dissipative PDEs, the theory of attracting sets and
inertial manifolds guarantee that these systems are actually <EM>finite</EM> 
dimensional.  See [Ruelle '89] and [Jolly, Kevrekidis &amp; Titi '90]
for examples.
In fact, numerical simulations of certain PDEs suggest that they
have relatively <EM>low</EM>-dimensional attractors.
It is data coming from such low-dimensional systems, either PDEs or
laboratory experiments, with which this thesis is primarily concerned.

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt3">
	 <B>3. Dynamical Systems Concepts</B>
</a>

<P>
In this chapter we present a few key concepts from
dynamical systems theory.
Within this mathematical theory, we hope to gain some understanding
of nonlinear systems by examining their spatiotemporal data.
An underlying assumption throughout this chapter is that the 
temporal dynamics of
the systems being studied has relatively few degrees of 
freedom - on the order of ten or less.  

<P>
A dynamical system is a way of describing the state of some system
as it evolves in time.  We illustrate via examples and simultaneously
introduce related concepts and terminology.

<P>
Dynamical systems are commonly described by ordinary differential 
equations (ODEs) of the form:
<BR><P></P>

<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
\frac{d{\bf x}}{dt} = {\bf f}({\bf x},t;\mu)
\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="50%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="MIDDLE"><IMG
 WIDTH="105" HEIGHT="51"  BORDER="0"
 SRC="eqn3.1.gif"
 ALT="$\displaystyle \frac{d{\bf x}}{dt} = {\bf f}({\bf x},t;\mu)$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="right">
(3.1)</TD></TR>
</TABLE></DIV>

<BR CLEAR="ALL">
<P></P>
with the dependent variables 
<!-- MATH: ${\bf x} \in I\!\!R^n$ -->
<IMG
 WIDTH="56" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img5.gif"
 ALT="${\bf x} \in I\!\!R^n$">,
the independent variable 
<!-- MATH: $t \in I\!\!R$ -->
<IMG
 WIDTH="44" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img6.gif"
 ALT="$t \in I\!\!R$">,
and possible free parameters

<!-- MATH: $\mu \in I\!\!R^m$ -->
<IMG
 WIDTH="60" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img7.gif"
 ALT="$\mu \in I\!\!R^m$">.
The space of dependent variables is called the <EM>phase space</EM> (or state
space) of a dynamical system.

<P>
The extensive theory of differential equations 
has an important role in the study
of dynamical systems.  For questions concerning the 
existence and uniqueness of solutions to (3.1), as well as other 
theoretical issues, see [Waltman '86].
When one tries to understand the asymptotic dynamics of differential equations,
it is assumed that enough time has elapsed so that
all transient behavior has died out.

<P>
We now describe how the study of dynamical systems makes use of geometric
concepts.  Since geometric descriptions are usually more intuitive than 
algebraic ones, the ideas are more easily grasped.

<P>
Every system of the form (3.1) implies a related <EM>vector field</EM>.
The idea is simple.  To each point in the phase space, we assign an
instantaneous velocity vector obtained from the right-hand side of the system.  
A <EM>solution</EM> (also, orbit or trajectory) of (3.1)
is a curve, 
<!-- MATH: ${\bf x}(t):
I\!\!R\rightarrow 
I\!\!R^n$ -->
<IMG
 WIDTH="108" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img8.gif"
 ALT="${\bf x}(t):
I\!\!R\rightarrow
I\!\!R^n$">,
whose tangent vector at any given <I>t</I> is the vector 
<!-- MATH: ${\bf f(x)}$ -->
<IMG
 WIDTH="33" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img9.gif"
 ALT="${\bf f(x)}$">
(at
the point <IMG
 WIDTH="13" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img10.gif"
 ALT="${\bf x}$">).  Hence, we can compute a solution by integrating the
right-hand side of the system.
A solution, 
<!-- MATH: ${\bf x}(t)$ -->
<IMG
 WIDTH="31" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.gif"
 ALT="${\bf x}(t)$">,
will depend on an
initial condition, <IMG
 WIDTH="20" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img12.gif"
 ALT="${\bf x}_0$">,
and on the free 
parameters <IMG
 WIDTH="13" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.gif"
 ALT="$\mu$">.

<P>
We illustrate with a simple example.
Consider the 2<I>D</I> autonomous<A NAME="tex2html1"
 HREF="footnode.html#foot118"><SUP>1</SUP></A>, linear system:
<!------------------------->
<DIV ALIGN="CENTER">
<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="50%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="MIDDLE">
<IMG
 WIDTH="39" HEIGHT="27"
 SRC="xdot_eq_y.gif"
 ALT="{\dot x} = y">
</DIV><P></P>

<DIV ALIGN="CENTER">
<IMG
 WIDTH="50" HEIGHT="30"
 SRC="ydot_eq_negx.gif"
 ALT="{\dot y} = -x">
 </TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=10 ALIGN="right">
(3.2)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL">
<P></P>
<!------------------------->
<P>
The vector field and a solution of (3.2) are shown in Figure 3.1.
<p>
<center>
<img src="center.gif"><br>
<p>
Figure 3.1 Vector field and a solution of (3.2).
</center>
<BR>
<BR CLEAR="ALL">
<P>
For system (3.2), we see that all solutions are periodic.  For any  

<!-- MATH: ${\bf x}_0 \neq (0,0)$ -->
<IMG
 WIDTH="77" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img17.gif"
 ALT="${\bf x}_0 \neq (0,0)$">,

<!-- MATH: ${\bf x}(t)$ -->
<IMG
 WIDTH="31" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img11.gif"
 ALT="${\bf x}(t)$">
rotates forever about the origin. 
The origin is a <EM>fixed point</EM> (also critical point or equilibrium).
Any solution starting at a fixed point will remain there.
The particular type of fixed point in this example is called a <EM>center</EM>.

<P>
To find all fixed points of a system, one solves for the zeros of the
system:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="88" HEIGHT="28"
 SRC="img18.gif"
 ALT="{\bf f}({\bf x},t;\mu) = 0">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
The study of dynamical systems has a primary goal - to  understand the
asymptotic (long-term) behavior of its
solutions in a qualitative way.  For example, 
solutions might: (1) approach some fixed point, (2) ``blow up'' after
some finite time, (3) approach some periodic solution, or (4) follow some other
complicated motion.

<P>
Typically, solutions of a system will undergo qualitative changes as
one of the system's free parameters, <IMG
 WIDTH="20" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img19.gif"
 ALT="${\mu}_j$">,
is varied.
Such a parameter is called a <EM>bifurcation parameter</EM>.  Understanding 
these qualitative changes in the geometry of solutions in phase space 
is the subject of bifurcation theory [Wiggins '90].

<P>
Minor algebraic modifications to (3.2) lead to qualitatively different
solution structures in phase space.  For a slightly more complicated
example, consider the coupled linear 2<I>D</I> system:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="111" HEIGHT="51"
 SRC="img20.gif"
 ALT="\begin{eqnarray*}{\dot x} &=& -x + 2y \\
{\dot y} &=& -2x - y
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">This system results in the fixed point 
at the origin to become a <EM>global attractor</EM>.
The fixed point is called a <EM>sink</EM>.  
The vector field and a spiraling solution toward the 
origin are shown in Figure 3.2.
<p>
<center>
<img src="spsink.gif"><br>
<p>
Figure 3.2 Vector field with a sink at <b>(0,0)</b>.
</center>

<P>
The origin, still a fixed point, is repelling in the system:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="97" HEIGHT="51"
 SRC="img21.gif"
 ALT="\begin{eqnarray*}{\dot x} &=& x - 2y \\
{\dot y} &=& 2x + y
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">This type of fixed point is called a <EM>source</EM>.  
The vector field and an outward spiraling solution are shown in Figure 3.3.
<p>
<center>
<img src="spsource.gif"><br>
<p>
Figure 3.3 Vector field with a source at <b>(0,0)</b>.
</center>
<BR CLEAR="ALL">
<P>
An example of a fixed point being both attracting (stable) and repelling 
(unstable) - in 
different directions, is given by the system: 
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="111" HEIGHT="51"
 SRC="img23.gif"
 ALT="\begin{eqnarray*}{\dot x} &=& -x + 2y \\
{\dot y} &=& 2x + y
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">Here, the fixed point at the origin is called a <EM>saddle</EM>.
The vector field is shown in Figure 3.4.
<p>
<center>
<img src="saddle.gif"><br>
<p>
Figure 3.4 Vector field with a saddle at <b>(0,0)</b>.
</center>

<BR CLEAR="ALL">
<P>
From the theory of ODEs, we know that the <EM>stability</EM> of fixed points of 
linear systems can
be determined from the eigenvalues of the coefficient matrix of the
right-hand side.  In the above systems, the sink is asymptotically stable,
the center is stable (but not asymptotically), and the source and saddle
are unstable.

<P>
Nonlinear systems are more likely to  be used as models for real-life systems.
They are more complex than linear systems and give rise to more interesting 
phase space structures than fixed points.
For example, in 2<I>D</I> (and higher) nonlinear systems, one can find
<EM>limit cycles</EM>.
A limit cycle is a closed, periodic orbit which can be either attracting
or repelling.  As an example, the following nonlinear system
contains an attracting limit cycle.  The system
is a model for electrical circuits whose resistive properties change
according to the supplied current.
It is known as the van der Pol system:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="146" HEIGHT="51"
 SRC="img25.gif"
 ALT="\begin{eqnarray*}{\dot x} &=& y - (x^3 - \mu x) \\
{\dot y} &=& -x
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">The parameter <IMG
 WIDTH="13" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img13.gif"
 ALT="$\mu$">
is a bifurcation parameter.  When <IMG
 WIDTH="42" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img26.gif"
 ALT="$\mu < 0$">,
the origin
is a global attractor.  When <IMG
 WIDTH="42" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img27.gif"
 ALT="$\mu > 0$">,
the origin loses stability, becoming
a repellor, and an attracting limit cycle is created.
The point at which this happens
is known as a <EM>Hopf bifurcation point</EM>.  A limit cycle of this system, for
<IMG
 WIDTH="42" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img28.gif"
 ALT="$\mu = 1$">,
is shown in Figure 3.5.
A solution starting near the origin is attracted outward to the
limit cycle, whereas a solution starting outside the limit cycle is attracted
inward to it.  The asymptotic solutions, <I>x</I>(<I>t</I>) and <I>y</I>(<I>t</I>), would be
sinusoidal as plotted in Figure 3.6.
<p>
<center>
<img src="vdp.gif"><br>
<p>
Figure 3.5 An attracting limit cycle.
</center>
<p>
<center>
<img src="vdp_xy.gif"><br>
Figure 3.6 <i>x(t), y(t)</i> of limit cycle.
</center>
<P>
The Poincar&#233;-Bendixson theorem is useful for guaranteeing the
existence of a limit cycle in 2<I>D</I> autonomous systems of ODEs
[Hirsch &amp; Smale '74].  The stability of fixed points 
becomes a local property for nonlinear systems.  One must linearize the
system about a fixed point to determine that fixed point's stability.
See [Waltman '86] for this important theory.

<P>
There are other closed, but nonperiodic, orbits which
occur in phase space.
A <EM>homoclinic orbit</EM> is one which leaves the neighborhood of a fixed
point along an
unstable direction and loops back on itself along a stable direction. 
A <EM>heteroclinic orbit</EM> connects two (or more)
different fixed points together via their stable and unstable manifolds
as illustrated in Figure 3.7.
<center>
<img src="homo.gif">
<img src="hetero.gif"><br>
Figure 3.7 A homoclinic and heteroclinic orbit.
</center>
<P>
So that one does not conclude <IMG
 WIDTH="27" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img31.gif"
 ALT="$I\!\!R^n$">
is the only possible phase space, 
we describe
a classic example of a dynamical system - the pendulum.
This physical system is illustrated in Figure 3.8.  It consists of a bob
<p>
<center>
<img src="pendulum_small.gif"><br>
Figure 3.8 The simple pendulum.
</center>
<P>
attached to a rigid rod which rotates around an axis.  The state of the
system is most naturally described by it phase and angular velocity:  
<IMG
 WIDTH="11" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img32.gif"
 ALT="$\theta$">
and 
<!-- MATH: $\omega={\dot \theta}$ -->
<IMG
 WIDTH="43" HEIGHT="18" ALIGN="BOTTOM" BORDER="0"
 SRC="img33.gif"
 ALT="$\omega={\dot \theta}$">,
with 
<!-- MATH: $0 \leq \theta \leq 2\pi$ -->
<IMG
 WIDTH="79" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img34.gif"
 ALT="$0 \leq \theta \leq 2\pi$">.
<BR>
<BR CLEAR="ALL">The dynamics are given by the system:

<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="157" HEIGHT="63"
 SRC="img36.gif"
 ALT="\begin{eqnarray*}{\dot \theta} &=& \omega \\
{\dot \omega} &=& -\frac{1}{l}sin\theta - \frac{k}{m}\omega
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
The phase space is therefore an infinite cylinder: 
<!-- MATH: $S^1\times I\!\!R$ -->
<IMG
 WIDTH="56" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img37.gif"
 ALT="$S^1\times I\!\!R$">.
For plotting convenience, however, this cylinder can be mapped into
<IMG
 WIDTH="26" HEIGHT="17" ALIGN="BOTTOM" BORDER="0"
 SRC="img38.gif"
 ALT="$I\!\!R^2$">
with <IMG
 WIDTH="11" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img32.gif"
 ALT="$\theta$">
being <IMG
 WIDTH="21" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.gif"
 ALT="$2\pi$">-periodic along the horizontal
axis.  Solutions in this phase space for the undamped system (<I>k</I>=0) 
are shown
in Figure 3.9 and for the damped system (<I>k</I>&gt;0) in Figure 3.10.
<p>
<center>
<img src="pendundamp.gif"><br>
<p>
Figure 3.9 Solutions of the undamped pendulum.
</center>
<p>
<center>
<img src="penddamp.gif"><br>
<p>
Figure 3.10 Solutions of the damped pendulum.
</center>

<p>
The base of each vector in the vector field serves as an initial value
for a solution.
In the undamped system, the position <IMG
 WIDTH="40" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img41.gif"
 ALT="$\theta=0$">
is locally a center.
This means that for small enough angular velocity, <IMG
 WIDTH="14" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img42.gif"
 ALT="$\omega$">,
the pendulum
will swing back and forth about <IMG
 WIDTH="40" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img41.gif"
 ALT="$\theta=0$">
without ever making 
a full revolution.
However, for too large angular velocity, the pendulum will forever revolve
about its axis in only one direction.
In the damped system, <IMG
 WIDTH="40" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img41.gif"
 ALT="$\theta=0$">
becomes a sink (locally) and any nearby
orbits get attracted to this fixed point.
The point 
<!-- MATH: $\theta=\pi$ -->
<IMG
 WIDTH="42" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img43.gif"
 ALT="$\theta=\pi$">
is a saddle.  A homoclinic orbit would leave from and
return to this point.

<P>
Going to three-dimensional systems, even more interesting dynamics and
geometric structures occur in phase space.  We illustrate with a well known
system given by [Lorenz '63]:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="135" HEIGHT="75"
 SRC="img44.gif"
 ALT="\begin{eqnarray*}{\dot x} &=& \sigma(y - x) \\
{\dot y} &=& {\rho}x - y - xz \\
{\dot z} &=& {\beta}z + xy
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">This system is an over-simplified model for convection.  It is also a model
for a perfectly tuned two-level laser [Newell &amp; Moloney '92]. 
The dynamics and bifurcations of the Lorenz
system are discussed in [Sparrow '82].
For particular values of the bifurcation parameters, 

<!-- MATH: $\sigma,\rho$ -->
<IMG
 WIDTH="27" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img45.gif"
 ALT="$\sigma,\rho$">,
and <IMG
 WIDTH="13" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img46.gif"
 ALT="$\beta$">,
an interesting attractor appears in
phase space.  Unlike a fixed point or a limit cycle, this attractor has a
more complicated geometric structure.  It is a three-dimensional ``figure-8''
shape.  However, the attractor is not a closed orbit.  Rather, it has some
``thickness''.
We show a solution captured within this attracting structure in Figure 3.11.
This idea of having a deterministic system which results in complex, yet
coherent, dynamics is the backdrop for <EM>chaos theory</EM> [Wiggins '88]
which has been popularized in [Gleick '87].
<p>
<center>
<img src="lorenz_xy.small.gif"><br>
<img src="lorenz_yz.small.gif"><br>
<p>
Figure 3.11 Lorenz attractor projected onto the <i>xy</i> plane (top) and
<i>yz</i> plane (bottom).
</center>
<BR>
<BR CLEAR="ALL">
<P>
It was stated earlier that a primary goal of studying dynamical systems
is to understand the long-term dynamics in a qualitative way.  The Lorenz
attractor offers a good example.  We know that for certain parameter values
of the system, solutions will asymptotically remain confined to the subspace in
which the attractor lies.  This raises another key concept in dynamical
systems theory - the <EM>dimension</EM> of attractors.  It is for this topic
that dynamical systems requires the notion of fractals and 
fractional dimensions [Barnsley '88] and [Farmer, Ott &amp; Yorke '83].

<P>
Being able to compute the dimension of an attractor and to construct
the attractor, using empirical data
from a dynamical system, has been an active area of research 
[Broomhead &amp; King '85], [Sauer, Yorke &amp; Casdagli '91], [Kostelich '92], 
and [Gibson, Farmer, Casdagli &amp; Eubank '92].
These methods typically operate on a time series of some scalar
measurement of the system as mentioned in Chapter 1.
This is inherently different from the 
analysis treated in
this thesis which deals with spatiotemporal data,
as discussed in Chapter 2.

<P>
In conclusion, we want to bridge the gap between the low-dimensional
dynamical systems as presented in this chapter and the 
infinite-dimensional PDEs as discussed in Chapter 2.
This is done via a Gal&#235;rkin projection.
Assuming the solution to a PDE can be approximated by the
expansion:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="157" HEIGHT="56"
 SRC="img48.gif"
 ALT="\begin{eqnarray*}u(x,t) \approx \sum_{i=1}^N a_i(t)\phi_i(x)
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where the <IMG
 WIDTH="18" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img49.gif"
 ALT="$\phi_i$">
are known spatial structures, then the time-dependent
coefficients, <I>a</I><SUB><I>i</I></SUB>(<I>t</I>), are obtained as solutions of an N-dimensional system
of ODEs, known as a Gal&#235;rkin projection:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="58" HEIGHT="28"
 SRC="img50.gif"
 ALT="\begin{eqnarray*}{\dot {\bf a}} = {\bf f}({\bf a})
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
We then need to make analogous classifications of a PDE's asymptotic
dynamics with those of a system of ODEs as presented in this chapter.
A fixed point corresponds to a stable spatial structure of a PDE.
A limit cycle corresponds to some oscillating spatial structure, for
example, either a standing wave or a travelling wave.
And chaotic (``strange'') attractors which are low-dimensional should
correspond to spatiotemporal complexity generated by the temporal
evolution of only a few coherent spatial structures.

<P>
This chapter has presented some basic definitions and concepts of 
dynamical systems.
For a visual and intuitive introduction, see 
[Abraham &amp; Shaw '84].  
A graphical environment for experimenting with dynamical systems on 
a personal computer can be found in [Ko&#231;ak '86].
Other computer packages which offer a
more sophisticated analysis of dynamical systems are [Guckenheimer &amp; Kim '90]
and [Yorke '90].
For a thorough and mathematically rigorous treatment of this
subject, see [Guckenheimer &amp; Holmes '83] and [Wiggins '90].

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt4">
	 <B>4. Karhunen-Lo&#232;ve Decomposition</B>
</a>

<P>
The method of analysis at the core of this thesis appears in various guises
and is known by different names - depending 
on the area of application.  In image processing, it is known as 
the <EM>Hotelling transform</EM>
[Gonzalez &amp; Wintz '87].  In pattern recognition,
the name <EM>principal component analysis</EM>
is commonplace.  Both application areas typically analyze an ensemble of 
static data.  For example, consider an image from a satellite's camera.
The same image will be transmitted repeatedly, but each version
will be slightly noise-contaminated due to Earth's atmosphere.
A subsequent analysis is performed to remove, or at least reduce, the noise.

<P>
The method is also known as the <EM>Karhunen-Lo&#232;ve</EM> (K-L) 
<EM>decomposition</EM>.
However, in the context of this thesis, the data to be analyzed will be
temporally evolving rather than being an ensemble.  

<P>
Another name for this method is <EM>proper orthogonal decomposition</EM> (POD).
[Lumley '67] proposed that the method be used in fluid flow analysis.
He suggested that it could provide an unbiased identification of coherent
structures in turbulent flow.
See [Aubry, Holmes, Lumley &amp; Stone '88] for an application.

<P>
Regardless of the name and the application, the method is 
essentially the same.  It is based on second-order statistical properties
[Lo&#232;ve '55] which result in a set of optimal eigenfunctions.
We now present the theory behind the decomposition.

<P>
First, we assume data to be a set of (real) random vectors:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="53" HEIGHT="28"
 SRC="img51.gif"
 ALT="\begin{eqnarray*}\{{\bf X}_i\}_{i=1}^M
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where 
<!-- MATH: ${\bf X} = [x_1,x_2, \ldots ,x_N]^T$ -->
<IMG
 WIDTH="153" HEIGHT="34" ALIGN="MIDDLE" BORDER="0"
 SRC="img52.gif"
 ALT="${\bf X} = [x_1,x_2, \ldots ,x_N]^T$">.
In the classical theory from statistical pattern recognition [Fukunaga '90],
one would then need to characterize each random vector with a probability
density function.  This would be used to define, via integration, the
mean of a random vector.
Finally, the covariance matrix, indicating
the dispersion of the vectors' distribution, could be calculated.
It is this matrix which provides the information that we want.
Specifically, we wish to know its eigenvalues and eigenvectors.

<P>
These theoretical calculations are approximated in actual computations 
from the given set of vectors.  The mean is computed as:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="102" HEIGHT="56"
 SRC="img53.gif"
 ALT="\begin{eqnarray*}\overline{{\bf X}} = \frac{1}{M} \sum_{i=1}^M {\bf X}_i
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
For convenience, we then compute an additional sequence of vectors which
have zero mean:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="245" HEIGHT="27"
 SRC="img54.gif"
 ALT="\begin{eqnarray*}\hat{{\bf X}}_i = {\bf X}_i - \overline{{\bf X}},\hspace*{0.5in} i=1,\ldots,M
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">The 
<!-- MATH: $\hat{{\bf X}}_i$ -->
<IMG
 WIDTH="22" HEIGHT="38" ALIGN="MIDDLE" BORDER="0"
 SRC="img55.gif"
 ALT="$\hat{{\bf X}}_i$">
are called the caricature vectors.

<P>
The covariance matrix is then approximated by:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="125" HEIGHT="56"
 SRC="img56.gif"
 ALT="\begin{eqnarray*}{\bf C} = \frac{1}{M} \sum_{i=1}^M \hat{{\bf X}}_i \hat{{\bf X}}_i^T
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
We point out that the covariance matrix is an 
<!-- MATH: $N \times N$ -->
<IMG
 WIDTH="52" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img57.gif"
 ALT="$N \times N$">matrix, where <I>N</I> is the spatial resolution of a vector.
For large <I>N</I>, this matrix can become too large for practical computation.
However, by using the method of snapshots, as described by [Sirovich '87a],
the computation becomes more tractable.
Using this method, the covariance matrix becomes:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="270" HEIGHT="29"
 SRC="img58.gif"
 ALT="\begin{eqnarray*}{\bf C}_{ij} = \langle \hat{{\bf X}}_i,\hat{{\bf X}}_j \rangle ,
\hspace*{0.5in} i,j=1,\ldots,M
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where 
<!-- MATH: $\langle\cdot,\cdot\rangle$ -->
<IMG
 WIDTH="32" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img59.gif"
 ALT="$\langle\cdot,\cdot\rangle$">
denotes the usual Euclidean inner product.
The matrix is now 
<!-- MATH: $M \times M$ -->
<IMG
 WIDTH="57" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img60.gif"
 ALT="$M \times M$">
instead of 
<!-- MATH: $N \times N$ -->
<IMG
 WIDTH="52" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img57.gif"
 ALT="$N \times N$">
and, assuming <I>M</I>&lt;<I>N</I>,
we can compute its eigenvalues and eigenvectors more easily. 

<P>
Since the covariance matrix is symmetric, we know that its 
eigenvalues, <IMG
 WIDTH="18" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img61.gif"
 ALT="$\lambda_i$">,
are nonnegative and its
eigenvectors, <IMG
 WIDTH="18" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img49.gif"
 ALT="$\phi_i$">,

<!-- MATH: $i=1, \ldots, M$ -->
<IMG
 WIDTH="90" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.gif"
 ALT="$i=1, \ldots, M$">,
form a complete orthogonal set [Strang '76].

<P>
The orthogonal eigenfunctions of the data are defined as:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="272" HEIGHT="56"
 SRC="img63.gif"
 ALT="\begin{eqnarray*}{\Psi}^{[k]} = \sum_{i=1}^M {\phi}_i^{[k]} \hat{\bf X}_i,
\hspace*{0.5in} k=1,\ldots,M
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where 
<!-- MATH: ${\phi}_i^{[k]}$ -->
<IMG
 WIDTH="28" HEIGHT="41" ALIGN="MIDDLE" BORDER="0"
 SRC="img64.gif"
 ALT="${\phi}_i^{[k]}$">
is the <I>i</I>-th component of the <I>k</I>-th eigenvector.
Lumley refers to these eigenfunctions as coherent structures of the data.
Whether or not they would appear as spatial structures in a laboratory
experiment is questionable [Sirovich '87b].  Nevertheless, there is cause to
believe that they will be present at least indirectly.  Perhaps
an actual structure will consist of a linear combination of eigenfunctions.

<P>
A characterization of the eigenfunctions is that they form an
optimal basis for the expansion of a spatiotemporal dataset:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="160" HEIGHT="56"
 SRC="img65.gif"
 ALT="\begin{eqnarray*}u(x,t) \approx \sum_{i=1}^K a_i(t)\Psi_i(x)
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">by minimizing the <I>L</I><SUP>2</SUP> norm of the error.

<P>
The ``energy'' of the data is defined as being the sum of the
eigenvalues of the covariance matrix:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="72" HEIGHT="56"
 SRC="img66.gif"
 ALT="\begin{eqnarray*}E = \sum_{i=1}^M {\lambda}_i
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">To each eigenfunction we assign an energy percentage based on
the eigenfunction's associated eigenvalue:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="60" HEIGHT="39"
 SRC="img67.gif"
 ALT="\begin{eqnarray*}E_k = \frac{\lambda_k}{E}
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">Assuming the eigenvalues are sorted largest to smallest, we have an
ordering of the eigenfunctions from most to least energetic .

<P>
Finally, we can reconstruct any sample vector using the eigenfunctions:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="131" HEIGHT="56"
 SRC="img68.gif"
 ALT="\begin{eqnarray*}{\bf X} = \overline{{\bf X}} + \sum_{i=1}^M{a_i {\Psi}^{[i]}}
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where the coefficients are computed from the projection of the sample vector
onto an eigenfunction:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="122" HEIGHT="54"
 SRC="img69.gif"
 ALT="\begin{eqnarray*}a_i = \left( \frac{\hat{\bf X} \cdot {\Psi}^{[i]}}
{{\Psi}^{[i]} \cdot {\Psi}^{[i]}} \right)
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">Using only the first <I>K</I> (<I>K</I>&lt;<I>M</I>) most energetic eigenfunctions, we 
can construct an approximation to the data:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="131" HEIGHT="56"
 SRC="img70.gif"
 ALT="\begin{eqnarray*}{\bf X} \approx \overline{{\bf X}} + \sum_{i=1}^K{a_i {\Psi}^{[i]}}
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
As a final note, we mention that the results of the above K-L decomposition can 
also be obtained via singular value decomposition (SVD).
See [Strang '76] for an introduction to SVD.
Using this method, one would not compute the 
<!-- MATH: $M \times M$ -->
<IMG
 WIDTH="57" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img60.gif"
 ALT="$M \times M$">
covariance matrix.
Instead, an 
<!-- MATH: $N \times M$ -->
<IMG
 WIDTH="54" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img71.gif"
 ALT="$N \times M$">
rectangular matrix is generated:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="121" HEIGHT="34"
 SRC="img72.gif"
 ALT="\begin{eqnarray*}\left[ \hat{\bf X}_1,\hat{\bf X}_2, \ldots ,\hat{\bf X}_M \right]
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">The singular values, 
<!-- MATH: ${\sigma}_i$ -->
<IMG
 WIDTH="18" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img73.gif"
 ALT="${\sigma}_i$">,

<!-- MATH: $i=1,\ldots,M$ -->
<IMG
 WIDTH="90" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img62.gif"
 ALT="$i=1, \ldots, M$">
(assuming <I>M</I>&lt;<I>N</I>)
and singular vectors of this matrix are then computed.
Using the fact that 
<!-- MATH: ${\sigma}_i^2 = \lambda_i$ -->
<IMG
 WIDTH="56" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img74.gif"
 ALT="${\sigma}_i^2 = \lambda_i$">,
the computations would
proceed as above.
An application and discussion of the SVD method for a PDE modeling
fluid motion can be found in [Newell, Rand &amp; Russell '88].
A comparison of SVD and K-L, in terms of
numerical accuracy, is discussed in [Mees, Rapp &amp; Jennings '87].
<p>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt5">
	 <B>5. Overview of <EM>kltool</EM></B>
</a>
<p>
We now describe the computer implementation of the
Karhunen-Lo&#232;ve (K-L) decomposition
as explained in the previous chapter.  
First, we discuss some implementation
issues and present an overview of capabilities of <EM>kltool</EM>.
We then describe specific applications of <EM>kltool</EM> using
examples in Chapter 6.

<P>

	 <EM>5.1. Implementation</EM>

<P>
Pragmatic considerations determined how <EM>kltool</EM> is implemented.
We wanted hardware which could render high-quality graphics and 
perform reasonably fast computations.  Also, since we wanted to make the
tool available to others, selecting a widely available platform was 
important.
The Silicon Graphics Personal
Iris was selected for the initial implementation primarily because of its
high-quality 3<I>D</I> graphics.

<P>
On the software side,
the tool is implemented in the C programming language.  All graphics are
performed using the Graphics Library, a set of graphics routines available on
Silicon Graphics workstations.  The numerical decomposition routines
are obtained from public domain software libraries.

<P>

	 <EM>5.2. Functionality</EM>

<P>
The functionality of <EM>kltool</EM> generally adheres to the current trend of 
workstation applications: interacting with the user via popup menus and
dialog boxes and displaying results in multiple windows on the screen.
These nontrivial issues are discussed in [Shneiderman '80].

<P>
A schematic overview of <EM>kltool's</EM> functionality is depicted in Figure
5.1.  We briefly describe the function of each operation:
<p>
<center>
<img src="overview.gif"><br>
Figure 5.1 Overview of <i>kltool</i>.
</center>
<P>
<BR CLEAR="ALL">
<P>
<DL>
<DT><STRONG>Input</STRONG>
<DD>Reads the dataset of vectors (or some subset thereof).  
The user specifies the spatial dimension of the vector 
(currently, only one- or two-dimensional vectors can be visualized)
and the vector resolution.
Examples of vectors might be 2<I>D</I> images from a
laboratory experiment, or discretized solutions of a PDE.
<DT><STRONG>Visualize</STRONG>
<DD>Allows viewing of the data.  One can select from a
list of viewing modes: wireframe or colored surface, scan or landscape.
<DT><STRONG>Select</STRONG>
<DD>Interactive selection of a subset of the data.
This feature can be quite important.  A dataset cannot be generated
arbitrarily but must contain the appropriate dynamics for the analysis
to be successful.  This point will become more clear in the examples.
<DT><STRONG>Mean</STRONG>
<DD>Compute and display the temporal average of the time series of
vectors.

<DT><STRONG>Covariance</STRONG>
<DD>Compute the covariance matrix, <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img76.gif"
 ALT="${\bf C}$">.
<DT><STRONG>Decompose</STRONG>
<DD>Perform K-L decomposition 
on <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img76.gif"
 ALT="${\bf C}$">.
(Alternatively, perform singular value decomposition). 
This results in a bar-chart display of energy percentages
of the eigenvalues (with a default cumulative of 95%).
The empirical eigenfunctions are then computed and displayed.
<DT><STRONG>Coefficients</STRONG>
<DD>Compute and display the time series coefficients of the
data projected onto an eigenfunction.
<DT><STRONG>Reconstruct</STRONG>
<DD>Compute and display an approximation to the original
data using <I>K</I> eigenfunctions.
<DT><STRONG>Error</STRONG>
<DD>Compute and display the absolute error between the data and its
approximation.
<DT><STRONG>Gal&#235;rkin Projection</STRONG>
<DD>Generate a Gal&#235;rkin projection,
a system of ODEs, for a PDE
using <I>K</I> eigenfunctions.  One must provide the PDE in a predefined syntax.
<EM>kltool's</EM> algorithm scans the symbolic PDE, 
searches for differentiation operators, 
<!-- MATH: $d(u,x,\cdot)$ -->
<IMG
 WIDTH="61" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img77.gif"
 ALT="$d(u,x,\cdot)$">,
and performs
the appropriate substitution and differentiation of <I>K</I> eigenfunctions.
An example is provided in Section 6.1.1.
</DL>
<P>
These functions are implemented in the software in a modular style.
This allows for errors to be easily 
traced and for new functionality to be easily incorporated.
For a more thorough description of the tool's operation, see
[Armbruster &amp; Heiland '92].

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt6">
	 <B>6. Examples</B>
</a>
<P>
We present four different examples of spatiotemporal data
and their resulting analyses
using <EM>kltool</EM>.
In the first example, data from a numerical simulation of a partial 
differential equation (PDE) is analyzed.
In the second example, we examine data from a fluid flow experiment.
The third example provides a recent analysis of dynamics
from a PDE simulation of fluid flow.  
In the last example, we analyze video data
from an experiment involving flame dynamics.

<P>
It is regrettable that the true dynamics found in these examples cannot be
better presented on paper.  A more natural medium would be video.
Nevertheless, we provide a depiction of the dynamics via landscape plots
(for 1<I>D</I> spatial vectors) and a set of frames (for 2<I>D</I> vectors).
We note, however, that the landscape plots represent only an appropriately
sampled subset of the entire dataset.  This is done to make the plots
more legible.

<P>

	 <EM>6.1. Kuramoto-Sivashinsky Equation</EM>

<P>
As an early test case for <EM>kltool</EM>, we chose a PDE which had
previously been studied using K-L analysis [Kirby &amp; Armbruster '91].
The PDE is known as the Kuramoto-Sivashinsky (K-S) equation:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="385" HEIGHT="38"
 SRC="img78.gif"
 ALT="\begin{eqnarray*}u_t + 4u_{xxxx} + \alpha(u_{xx} + \frac{1}{2}(u_x)^2) = 0,\hspace*{0.5in}
0\leq{x}\leq{2\pi}
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">We impose periodic boundary conditions: 
<!-- MATH: $u(0,t)=u(2\pi,t)$ -->
<IMG
 WIDTH="119" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img79.gif"
 ALT="$u(0,t)=u(2\pi,t)$">.

<P>
This equation is used as a model for flame fronts and thin viscous
fluid films, among other phenomena.  
Much of its spatiotemporal dynamics are known, 
through an exhaustive numerical study, and described in
[Hyman, Nicolaenko &amp; Zaleski '86].
As the bifurcation parameter, <IMG
 WIDTH="14" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="alpha.gif"
 ALT="$\alpha$">,
increases, the dynamics exhibit
a variety of interesting behavior, including: fixed points, traveling waves,
beating waves, homoclinic and heteroclinic orbits, and chaos.

<P>
For the purpose of our analysis, we chose two values for <IMG
 WIDTH="14" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="alpha.gif"
 ALT="$\alpha$">which produce distinctive dynamics.

<P>

	 <EM>6.1.1. Homoclinic Cycle</EM>

<P>
For 
<IMG ALIGN="BOTTOM" BORDER="0" SRC="alpha.gif">=17.75,
the K-S equation is known to have a homoclinic cycle.
In fact, these dynamics have been explained in an analytic framework 
[Armbruster, Guckenheimer &amp; Holmes '89]
and [Kevrekidis, Nicolaenko &amp; Scovel '90].
To verify that <EM>kltool</EM> could
correctly analyze this situation, we numerically computed a time series of
1<I>D</I> spatial 
solution vectors as input.  The solutions were discretized at a resolution
of 256 discrete points over the <IMG
 WIDTH="21" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.gif"
 ALT="$2\pi$">
domain.  We computed 400 snapshots
in time.  This dataset is depicted in Figures 6.1 and 6.2 and is characterized
by two bursts.
<center>
<img src="ks_17_data.small.gif"><br>
Figure 6.1 K-S (<img SRC="alpha.gif">=17.75) dataset landscape plot
</center>
<p>
<center>
<img src="ks_17_contour.small.gif"><br>
Figure 6.2 K-S (<img SRC="alpha.gif">=17.75) dataset contour plot
</center>

<P>
The K-L analysis results are shown in Figures 6.3 and 6.4.  Figure 6.3
shows a screen image from <EM>kltool</EM>.  In the upper-left window, the
dataset is plotted in landscape mode.  The view is from the front, meaning
that the spatial coordinate is horizontal and the temporal snapshots are 
going ``into'' the screen.  The vertical direction measures the amplitude
of the solutions, <I>u</I>(<I>x</I>,<I>t</I>).  In the upper-right, the dataset is displayed
as a color-coded surface from the side view,  meaning the horizontal axis
is now <I>t</I>.  This view highlights the bursting behavior of the data.
In the lower-left window, a bar-chart indicates the energy percentages
of the first
three eigenfunctions.  Recall that <EM>kltool</EM>, by default, will compute
enough eigenfunctions for at least 95% of the total energy.  We see that
the first eigenfunction (K-L mode), <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">,
has most (<IMG
 WIDTH="16" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img83.gif"
 ALT="$\approx$">
80%) of
the total energy.  In the lower-right window, <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
is plotted (scaled
independently of the dataset windows).
<!--------------------------------------------------------->
<p>
<center>
<! 1278x1022>
<a href=ks.17.gif><img src="ks.17.small.gif"></a>
<br>
Figure 6.3 K-S (<img SRC="alpha.gif">=17.75) data from front (x horiz)(u-l), side
(t horiz)(u-r), energy %s(l-l), <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">(l-r).<br>
(click on image for 1278x1022 enlargement)
</center>
<p>

<P>
Referring back to Figure 6.1, we see that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
represents the state of
the system before the first burst and after the second burst.  There is a
different state between bursts.  Therefore, in phase space concepts, we have
a heteroclinic orbit:  we leave one unstable fixed point, go to another, and
return to the original.  It so happens, however, that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
also represents
the steady state (fixed point) found between bursts.  It is simply a <IMG
 WIDTH="29" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img84.gif"
 ALT="$\pi/2$"><TD>&nbsp;</TD> phase-shifted copy of the other steady state.  
Typically, such fixed points would be structurally unstable and would not
occur in simulations.  However, for certain symmetric systems, of which this
system is an example, theory can explain why such fixed points are structurally
stable.  The special type of heteroclinic orbit, involving phase-shifted
fixed points, is called a homoclinic cycle.

<P>
Figure 6.4 shows the first three eigenfunctions: 
<!-- MATH: $\Psi_1,\Psi_2,\Psi_3$ -->
<IMG
 WIDTH="76" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img85.gif"
 ALT="$\Psi_1,\Psi_2,\Psi_3$">(each mapped into the amplitude range [-1,1]).
We perform a reconstruction of the dataset using only <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">.
<!---------------------------------------->
<p>
<center>
<img src="ks17.ef1_3.gif">
<br>
Figure 6.4 K-S (<img SRC="alpha.gif">=17.75) 
<img WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0" SRC="Psi1.gif">, 
<img WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0" SRC="Psi2.gif">, and
<img WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0" SRC="Psi3.gif">.
</center>
<p>

Figure 6.5 shows the original data (upper-left), the reconstruction 
(upper-right), and the absolute error (lower-left).  All three plots are drawn
to the same scale.  Notice that the error is most significant during the
bursts, that is, during the excursion between steady states.

<P>
In Figure 6.6, we show a reconstruction of the data using 
<!-- MATH: $\Psi_1,\Psi_2,$ -->
<IMG
 WIDTH="54" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img87.gif"
 ALT="$\Psi_1,\Psi_2,$">and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">.
Notice that the error is now much smaller than before.  This
indicates that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
is involved in the bursts, as one might have suspected.

<!------------------------------------------------------>
<p>
<center>
<a href=ks.17.a2.gif><img src="ks.17.a2.small.gif"></a>
<br>
Figure 6.5 K-S (<img SRC="alpha.gif">=17.75) dataset(u-l), reconstruction with
<img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">, <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(u-r), error(l-l), 
<img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<!------------------------------------------------------>
<p>
<center>
<a href=ks.17.a3.gif><img src="ks.17.a3.small.gif"></a>
<br>
Figure 6.6 K-S (<img SRC="alpha.gif">=17.75) dataset(u-l), reconstruction with
<img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">, <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">,<img SRC="Psi3.gif" ALIGN="MIDDLE" BORDER="0">(u-r), error(l-l), 
<img SRC="Psi3.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>

<!------------------------------------------------------>
<p>
Figure 6.7 shows the coefficients of 
<!-- MATH: $\Psi_1,\Psi_2,$ -->
<IMG
 WIDTH="54" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img87.gif"
 ALT="$\Psi_1,\Psi_2,$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
as
a time series.  
This
confirms the analysis results: <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
represents the unstable fixed points
(differing by a phase shift); <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
are dormant near the
fixed points, but get activated during the bursts.

<!------------------------------------------------------>
<p>
<center>
<img src="ks17.ts1_3.gif"><br>
Figure 6.7 K-S (<img SRC="alpha.gif">=17.75) Coefficients of <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">, <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">, and <img SRC="img88.gif" ALIGN="MIDDLE" BORDER="0">.
</center>
<p>

<P>
We use this dataset as an example on which to perform a Gal&#235;rkin 
projection.  First, we express the solution of the PDE 
as a linear combination of time-dependent coefficients and 
spatial eigenfunctions, resulting in a K-L Gal&#235;rkin expansion:
<p>
<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
u(x,t) \approx \sum_{i=1}^K{a_i(t)\Psi_i(x)}
\end{eqnarray} -->

<TABLE ALIGN="center" CELLPADDING="0" WIDTH="100%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="164" HEIGHT="65" BORDER="0"
 SRC="eqn6.1.gif"
 ALT="$\displaystyle u(x,t) \approx \sum_{i=1}^K{a_i(t)\Psi_i(x)}$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=70 ALIGN="left">
(6.1)</TD></TR>
</TABLE></DIV>
<BR CLEAR="ALL">
<P>
where <I>K</I> is the number of eigenfunctions chosen for the reconstruction.

<P>
The K-S PDE is then rewritten as:

<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
u_t = -4u_{xxxx} - \alpha(u_{xx} + \frac{1}{2}(u_x)^2)
\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="80%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="right"><IMG
 WIDTH="233" HEIGHT="49" BORDER="0"
 SRC="eqn6.2.gif"
 ALT="$\displaystyle u_t = -4u_{xxxx} - \alpha(u_{xx} + \frac{1}{2}(u_x)^2)$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=70 ALIGN="left">
(6.2)</TD></TR>
</TABLE></DIV>

<BR CLEAR="ALL">
<P></P>
Plugging (6.1) into (6.2) and differentiating, we obtain:

<DIV ALIGN="CENTER">

<!-- MATH: \begin{eqnarray}
\sum_{i=1}^K{{\dot a}_i\Psi_i} = -4\sum_{i=1}^K{{a_i}{\Psi_i}^{(4)}} - \alpha
(\sum_{i=1}^K{{a_i}{\Psi_i}''} + \frac{1}{2}(\sum_{i=1}^K{{a_i}{\Psi_i}'})^2)

\end{eqnarray} -->

<TABLE ALIGN="CENTER" CELLPADDING="0" WIDTH="80%">
<TR VALIGN="MIDDLE"><TD NOWRAP ALIGN="RIGHT"><IMG
 WIDTH="398" HEIGHT="65" ALIGN="MIDDLE" BORDER="0"
 SRC="eqn6.3.gif"
 ALT="$\displaystyle \sum_{i=1}^K{{\dot a}_i\Psi_i} = -4\sum_{i=1}^K{{a_i}{\Psi_i}^{(4...
...a
(\sum_{i=1}^K{{a_i}{\Psi_i}''} + \frac{1}{2}(\sum_{i=1}^K{{a_i}{\Psi_i}'})^2)$"></TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD>&nbsp;</TD>
<TD WIDTH=70 ALIGN="left">
(6.3)</TD></TR>
</TABLE></DIV>

<BR CLEAR="ALL">
<P></P>
where <IMG
 WIDTH="12" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img92.gif"
 ALT="${\dot a}$">
denotes differentiation with respect to <I>t</I>, <IMG
 WIDTH="20" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img93.gif"
 ALT="$\Psi'$">
is with
respect to <I>x</I>, and for conciseness, the independent variables are omitted.

<P>
Now we describe how the K-L Gal&#235;rkin projection is performed within
<EM>kltool</EM>.
We write each <IMG
 WIDTH="21" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img94.gif"
 ALT="$\Psi_i$">
as a Fourier expansion:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="140" HEIGHT="58"
 SRC="img95.gif"
 ALT="\begin{eqnarray*}\Psi_m = \sum_{k=-H}^H{{c_{k,m}}e^{ikx}}
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">(where <I>H</I> depends on the spatial discretization of <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img96.gif"
 ALT="$\Psi$">)
and recall the orthogonality condition of the 
<!-- MATH: $\{\Psi_i\}$ -->
<IMG
 WIDTH="37" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img97.gif"
 ALT="$\{\Psi_i\}$">:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="150" HEIGHT="45"
 SRC="img98.gif"
 ALT="\begin{eqnarray*}(\Psi_i,\Psi_j) = \left\{ \begin{array}{ll}
0 & $i$\space \neq $j$\space \\
C_i & $i=j$ \end{array} \right.
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">where <I>C</I><SUB><I>i</I></SUB> is a constant.

<P>
Multiplying (6.3) by 
<!-- MATH: $\Psi_j, j=1,\ldots,K$ -->
<IMG
 WIDTH="116" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img99.gif"
 ALT="$\Psi_j, j=1,\ldots,K$">
and integrating
from 0 to <IMG
 WIDTH="21" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.gif"
 ALT="$2\pi$">,
we obtain a Gal&#235;rkin projection, a system
of ODEs, for the PDE.
Appendix A shows the resulting system generated by <EM>kltool</EM>, using the
first three K-L modes.  This ODE system is then numerically integrated
<A NAME="tex2html2"
 HREF="footnode.html#foot551"><SUP>2</SUP></A> and the
time series solutions are plotted in Figure 6.8.
<p>
<center>
<img src="ks17.ode1_3.gif"><br>
Figure 6.8 K-S (<img SRC="alpha.gif">=17.75) Galerkin projection time series solutions.
</center>
<p>

Comparing these solutions with the time series of Figure 6.7, we see
the qualitative approximation that this synthesis offers.
A primary reason for the ``smoothing out'' of the spikes in the time series
plots is because <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
lacks a symmetric counterpart.
By symmetrizing the eigenfunctions, we should obtain better results.
It should be noted that cubic damping terms
were added to the ODE system to prevent blow-up solutions, as suggested in
[Kirby &amp; Armbruster '91].

<P>

	 <EM>6.1.2. Strange Mode</EM>

<P>
We now analyze a dataset from the K-S PDE for 
<!-- MATH: $\alpha=84.25$ -->
<IMG ALIGN="BOTTOM" BORDER="0" SRC="alpha.gif">=84.25.
We take 200 solution snapshots in time.
The dataset is depicted in Figure 6.9.
We know from
[Hyman, Nicolaenko &amp; Zaleski '86] and [Kirby &amp; Armbruster '92]
what an analysis should produce.
For <IMG ALIGN="BOTTOM" BORDER="0" SRC="alpha.gif">=72,
the PDE has a stable fixed point.  The fixed point is
shaped as a two-humped curve, one hump larger than the other, and is
called a ``strange'' fixed point.
At <IMG ALIGN="BOTTOM" BORDER="0" SRC="alpha.gif">=83.75,
this fixed point becomes unstable and undergoes a
Hopf bifurcation.  This results in an oscillation riding on top of the
larger hump as we see in Figure 6.9.
<p>
<center>
<img src="ks.84.data.gif"><br>
Figure 6.9 K-S (<img SRC="alpha.gif">=84.25) dataset landscape plot.
</center>

<BR CLEAR="ALL">
<P>
Figure 6.10 shows <EM>kltool's</EM> analysis of the given dataset.  In the
upper-left window, a color-coded surface of the data is viewed from the  
top.  This produces a color-contour plot with a horizontal <I>x</I>-coordinate 
and vertical <I>t</I>-coordinate.  The mean of the data, having the shape of
the fixed point at
<IMG ALIGN="BOTTOM" BORDER="0" SRC="alpha.gif">=72,
is plotted in the upper-right window.
The energy percentages are plotted in the lower-left, showing 
that the first three 
eigenfunctions contain 98%  of the total energy.  In the lower-right
window, the first eigenfunction, <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">,
is plotted.  This clearly
reveals the oscillation localized around the larger hump. 
<p>

<!------------------------------------------------------>
<center>
<a href=ks.84.gif><img src="ks.84.small.gif"></a>
<br>
Figure 6.10 K-S (<img SRC="alpha.gif">=84.25) color contours(u-l) mean(u-r),
energy %s(l-l), <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<P>
<!------------------------------------------------------>

We note that the eigenfunctions as computed would
indicate the dynamics of the data after
the mean is subtracted.  However, it is also possible to let <EM>kltool</EM>
perform a K-L analysis without subtracting the mean.  
It seems to be an open research problem to determine whether the first
eigenfunction for such a situation is in fact the mean.
For this dataset at least, the
first eigenfunction is essentially the mean of the data and it contains 
99.4% of the total energy.  The first three eigenfunctions are plotted in
Figure 6.11.  
<!-------------------------------------------------------------->
<p>
<center>
<img src="ks84.ef1_3.gif"><br>
Figure 6.11 K-S (<img SRC="alpha.gif">=84.25) <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0"> (=mean), <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">, and <img SRC="img88.gif" ALIGN="MIDDLE" BORDER="0">.
</center>
<!-------------------------------------------------------------->
<p>
By projecting the data onto these eigenfunctions, we obtain
the time series coefficients 
for 
<!-- MATH: $\Psi_1,\Psi_2,$ -->
<IMG
 WIDTH="54" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img87.gif"
 ALT="$\Psi_1,\Psi_2,$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
as shown in Figure 6.12. 
The time series for the coefficients of <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
is nearly constant and those
of the remaining eigenfunctions are oscillatory.
This suggests that the dynamics consist of a limit cycle encircling an
unstable, yet dominant, fixed point.
The limit cycle is essentially spanned by three K-L modes. 
Some recent research suggests that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
may in fact be a
travelling structure [Stone '92].
This would be believable judging from the upward slope of <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">'s
coefficients in Figure 6.12.
<!-------------------------------------------------------------->
<p>
<center>
<img src="ks84.ts1_3.gif"><br>
Figure 6.12 K-S (<img SRC="alpha.gif">=84.25) coefficients of <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">, <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">, and <img SRC="img88.gif" ALIGN="MIDDLE" BORDER="0">.
</center>

<P>

	 <EM>6.2. Couette-Taylor Flow</EM>

<P>
The Couette-Taylor experiment is widely known among fluid dynamicists.  The
apparatus consists of a cylindrical tank with a smaller, inner cylindrical
tank.  A fluid is contained between their two walls and the inner cylinder
rotates - inducing a flow of the fluid.  This flow 
has qualitatively different states, or flow patterns,
depending on the 
rotation speed of the inner cylinder [DiPrima &amp; Swinney '81].  Allowing both
cylinders to rotate, either in the same or opposite direction, 
causes different flow patterns to form.
Couette, a French scientist, was the first to experiment with this in the 
1880's.  In the 1920's, Taylor, a British mathematician, found some of the 
more interesting patterns.
An enlightening, non-mathematical discussion of this experiment is found in
[Stewart &amp; Golubitsky '92].

<P>
For the purpose of our analysis, we were provided with data obtained in
the following manner.  
The apparatus was photographed with a specially modified camera containing
a charge-coupled device (CCD) array.
A one-dimensional spatial vector of light intensity values in the azimuthal
direction was recorded at regular
time intervals, providing an indication of the system's flow pattern.
An illustration of the experiment is shown in Figure 6.13.  
<p>
<center>
<img src="couette_exp.gif"><br>
Figure 6.13 Schematic illustration of Couette-Taylor experiment.
</center>
<p>
<center>
<img src="ct2.gif"><br>
Figure 6.14 Couette-Taylor dataset.
</center>

<P>
The dataset consisted of 291 snapshot vectors, each with 891 
components whose values were proportional to the light intensity.
The dataset is depicted in Figure 6.14.
One can see from this plot that near both ends of the spatial vectors,
the amplitudes are constant in time.  
These values correspond to a boundary layer at the top and 
bottom of the Couette apparatus and are
not relevant to the fluid behavior in the middle portion of the cylinder.
Therefore, we make use of <EM>kltool's</EM> interactive data selection feature to
remove the data near the boundaries.  The resulting clipped vectors
are then analyzed.

<P>
First, we show analysis results using only the first 50 snapshots of the
dataset.  Figure 6.15 shows the 31-st snapshot in the data window 
(upper-left), the mean of the dataset (upper-right), the <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img96.gif"
 ALT="$\Psi$">
energy 
percentages (lower-left), indicating 12 are necessary for 95%, 
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
(lower-right).  After scanning through the snapshots<A NAME="tex2html3"
 HREF="footnode.html#foot400"><SUP>3</SUP></A>, we
observe that the data is quite ``noisy''.  This contributes to an analysis
which suggests the dynamics are not as low-dimensional as one might hope.
The ratio of 12 eigenfunctions to 50 snapshots indicates high-dimensional
phase space dynamics.
<!---------------------------------------------------->
<p>
<center>
<a href=couette1.gif><img src="couette1.small.gif"></a>
<br>
Figure 6.15 Couette-Taylor: snapshot(u-l) mean(u-r),
energy %s(l-l), <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>

<P>
In spite of these discouraging results, we still perform a reconstruction
of the data using the first <I>K</I> eigenfunctions.  Figure 6.16 shows a
color-contour plot of the original dataset (upper-left), a reconstruction
using the first two eigenfunctions (<I>K</I>=2) (upper-right), and the absolute
error (lower-right).  Figure 6.17 shows a reconstruction with <I>K</I>=6
and Figure 6.18 shows a reconstruction with <I>K</I>=12.  Notice that the
successive error plots contain less coherent structure, indicating a better
approximation with more eigenfunctions.
<!---------------------------------------------------->
<p>
<center>
<a href=couette2.gif><img src="couette2.small.gif"></a>
<br>
Figure 6.16 Couette-Taylor: dataset(u-l) reconstruction with <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">, <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(u-r),
energy %s(l-l), error(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<!---------------------------------------------------->
<P>
<center>
<a href=couette3.gif><img src="couette3.small.gif"></a>
<br>
Figure 6.17 Couette-Taylor: dataset(u-l) reconstruction with <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">-<img SRC="Psi6.gif" ALIGN="MIDDLE" BORDER="0">(u-r),
energy %s(l-l), error(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<!---------------------------------------------------->
<P>
<center>
<a href=couette4.gif><img src="couette4.small.gif"></a>
<br>
Figure 6.18 Couette-Taylor: dataset(u-l) reconstruction with <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">-<img SRC="Psi12.gif" ALIGN="MIDDLE" BORDER="0">(u-r),
energy %s(l-l), error(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>

<P>
Next, we perform an analysis of the entire dataset, consisting of 
291 snapshots.
The results indicate that 46 eigenfunctions are necessary to capture 
95% of the total energy.  Figure 6.19 shows the original dataset (upper) and
a reconstruction using only the first 10 eigenfunctions (lower).
Figure 6.20 shows a reconstruction using the first 25.
Depending on the desired goal, one might conclude that a resonable 
approximation can be obtained with fewer <IMG
 WIDTH="21" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img94.gif"
 ALT="$\Psi_i$">
than necessary for 95% of the energy.
Certainly, fewer eigenfunctions are necessary to
capture only the dominant (red) structures.
<!---------------------------------------------------->
<P>
<center>
<a href=ct_entire1.gif><img src="ct_entire1.small.gif"></a>
<br>
Figure 6.19 Couette-Taylor: entire dataset(top), reconstruction with <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">-<img SRC="Psi10.gif" ALIGN="MIDDLE" BORDER="0"> (bottom).
<br>
(click on image for 658x679 enlargement)
</center>
<P>
<!---------------------------------------------------->
<center>
<a href=ct_entire2.gif><img src="ct_entire2.small.gif"></a>
<br>
Figure 6.20 Couette-Taylor: entire dataset(top), reconstruction with <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">-<img SRC="Psi25.gif" ALIGN="MIDDLE" BORDER="0"> (bottom).
<br>
(click on image for 666x687 enlargement)
</center>
<P>

<P>

	 <EM>6.3. Kolmogorov Flow</EM>

<P>
This example, like the last, examines the topic of fluid flow.
Here, we do so via a PDE numerical simulation.

<P>
The notion of turbulence in fluid flow is still not understood.
There are, however, certain scenarios which seem to serve
as forerunners to turbulence [Eckmann '81].
One such scenario involves intermittent bursting of certain measured 
quantitites.  We analyze data of this type here.
For a mathematical introduction to fluid flow, see [Chorin &amp; Marsden '90].
A non-mathematical introduction to turbulence can be found in [Ruelle '91].

<P>
The PDE we consider is a version of the Navier-Stokes equation.
The 2<I>D</I> Kolmogorov flow is the solution of the 2<I>D</I> incompressible
Navier-Stokes equation subject to a force in the <I>x</I>-direction proportional
to <IMG
 WIDTH="36" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img104.gif"
 ALT="$\cos y$">:

<!-- MATH: $f=(\nu k_f^3 \cos k_f y,0)$ -->
<IMG
 WIDTH="138" HEIGHT="33" ALIGN="MIDDLE" BORDER="0"
 SRC="img105.gif"
 ALT="$f=(\nu k_f^3 \cos k_f y,0)$">.
It was introduced by Kolmogorov in the late 1950's as an example on 
which to study
transition to turbulence. For large enough viscosity <IMG
 WIDTH="12" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img106.gif"
 ALT="$\nu$">,
the only stable
flow is the plane parallel periodic shear flow 
<!-- MATH: $u_0 = ( k_f \cos k_f y,0)$ -->
<IMG
 WIDTH="136" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img107.gif"
 ALT="$u_0 = ( k_f \cos k_f y,0)$">,
usually called the ``basic Kolmogorov flow''. 
The macroscopic Reynolds number of the basic flow is found 
to be <IMG
 WIDTH="28" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img108.gif"
 ALT="$1/\nu$">.
This Reynolds number, <I>Re</I>, is used as a bifurcation parameter.

<P>
In a <IMG
 WIDTH="21" HEIGHT="13" ALIGN="BOTTOM" BORDER="0"
 SRC="img39.gif"
 ALT="$2\pi$">-periodic square domain, the equations are:
<BR><P></P>
<DIV ALIGN="CENTER">
<IMG
 WIDTH="449" HEIGHT="78"
 SRC="img109.gif"
 ALT="\begin{eqnarray*}u_t + u \cdot \nabla u + \nabla p &=&
\nu \nabla^2 u + f \\
\...
...&=& (\nu k_f^3 \cos k_f y,0) \hspace*{2cm} 0 \leq x,y \leq 2 \pi
\end{eqnarray*}">
</DIV><P></P>
<BR CLEAR="ALL">
<P>
Analytic results about the asymptotic solutions of this equation,
as well as some bifurcation results, can be found in [She '87].
More recently,
interesting transitions that occur at higher Reynolds number 
have been studied in [She &amp; Nicolaenko '90] and [Nicolaenko &amp; She '91].
They lead to sparsely distributed bursts in time for a fairly large range of
Reynolds number from a threshold of 
<!-- MATH: $Re \approx 20.5$ -->
<IMG
 WIDTH="72" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img110.gif"
 ALT="$Re \approx 20.5$">
to 

<!-- MATH: $Re \approx 120$ -->
<IMG
 WIDTH="68" HEIGHT="15" ALIGN="BOTTOM" BORDER="0"
 SRC="img111.gif"
 ALT="$Re \approx 120$">
(for <I>k</I><SUB><I>f</I></SUB>=8)<A NAME="tex2html4"
 HREF="footnode.html#foot410"><SUP>4</SUP></A>. 
The most striking feature of this transition is that the bursts generate 
substantial spatial disorder and drive developed turbulence.
Typically, near threshold, the dynamics follow long laminar regimes, 
interspersed with chaotic bursts. Intervals 
between bursts are not constant and fluctuate randomly. 
We analyze this near-threshold dynamics here.  Figure 6.21 illustrates
this behavior.  These plots are time series of the maximum magnitude of the
convective term.  Not plotted between the bursts are lengthy laminar regimes.
One might theorize that these bursts can be explained by homoclinic or 
heteroclinic orbits in phase space, as was the case for the 
Kuramoto-Sivashinsky PDE in Section 6.1.1.
Our goal is to compare laminar and burst data through a K-L analysis, 
looking for common spatial structures and trying to understand what is
responsible for the bursting dynamics.
<P>
<center>
<img src="burst3.gif"><br>
Figure 6.21 Intermittent bursts in Kolmogorov flow.
</center>
<P>

<P>
This example demonstrates how <EM>kltool</EM> can operate on various output
data generated by the same PDE simulation.
While the PDE numerical code integrates a stream function,
turbulence theory prefers to deal with vorticity.
We therefore analyze both stream and vorticity data, as well as the
the Fourier amplitudes generated by the simulation.
It should be noted that although vorticity is vector-valued, and <EM>kltool</EM>
does not analyze vectors of vector-valued data, the <I>x</I> and <I>y</I> components
of the vorticity are zero for 2<I>D</I> flow.  Hence, the vorticity vector is 
strictly in the <I>z</I>-direction, and it is this component which is analyzed.

<P>
We mention a preprocessing step that was necessary before K-L analysis 
could proceed.  For the Reynolds value of this example,
the physical solution data is traveling in the <I>x</I>-direction. Theoretical
analysis shows that the K-L eigenfunctions for a traveling wave become
sinusoids. However, we are not interested in the analysis
of the traveling wave, but want to treat it as one coherent structure,
and study the remaining dynamics in the data.  Hence,
we process our data first, calculate the wave speed and go into a co-moving
coordinate frame. 
We subtract the mean and perform our K-L analysis
on the remaining data, both for the laminar and the bursting regime.

<P>
The simulation data come from
a 
<!-- MATH: $64 \times 64$ -->
<IMG
 WIDTH="55" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.gif"
 ALT="$64 \times 64$">
pseudo spectral algorithm.

<P>

	 <EM>6.3.1. Stream Function Analysis</EM>

<P>
A K-L analysis on the laminar regime using 
the stream function data led to two eigenfunctions required for 95%
of the total energy.  These eigenfunctions are plotted in
Figure 6.22.
<P>
<center>
<img src="str29_1.small.gif"><br>
<img src="str29_2.small.gif"><br>
Figure 6.22 Kolmogorov(stream, laminar): (89.8% of energy)(top) and
(9.4% of energy)(bottom).
</center>
<P>

A reconstruction
of the data using just those two modes together with the mean gives an
almost perfect agreement between data and reconstruction. Using K-L on
the Fourier amplitudes, we capture almost the same amount of energy in the 
first few eigenfunctions. Figure 6.23 shows the time series of coefficients
for <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">,
indicating that they span a limit cycle.
<P>
<center>
<img src="str.lam.coefs.gif"><br>
Figure 6.23 Kolmogorov(stream, laminar): coefficients of <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0"> and <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">.
</center>
<P>

<P>
Analyzing the burst for the stream function data shows some surprising
results.  Here, three K-L modes are required to capture at least 95% 
of the energy.  These are plotted in Figures 6.24 and 6.25.
In addition, we show <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi8.gif"
 ALT="$\Psi_8$">
in Figure 6.25 to illustrate how an
eigenfunction, although containing very little energy, can contribute
to localized spatial dynamics - at a vortex in this case.
<P>
<center>
<img src="str30_1.small.gif"><br>
<img src="str30_2.small.gif"><br>
Figure 6.24 Kolmogorov(stream, burst): <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0"> (80.9% of energy)(top) and <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0"> (12.1% of energy)(bottom).
</center>
<P>
<center>
<img src="str30_3.small.gif"><br>
<img src="str30_8.small.gif"><br>
Figure 6.25 Kolmogorov(stream, burst): <img SRC="img88.gif" ALIGN="MIDDLE" BORDER="0"> (5.5% of energy)(top) and <img SRC="Psi8.gif" ALIGN="MIDDLE" BORDER="0"> (0.1% of energy)(bottom).
</center>
<P>

<BR CLEAR="ALL">The first mode, <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">,
in the laminar regime is roughly the same as <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">in the bursting regime. However, both <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
in the bursting
regime seem to be related to <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
in the laminar regime. 
It appears that they are real and imaginary parts of the same 
complex eigenfunction. This suggests that if a limit cycle lies in 
a real subspace,
then its unstable manifold is in the imaginary subspace. This was corroborated
by examining <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
in their Fourier representation.

<P>

	 <EM>6.3.2. Vorticity Analysis</EM>

<P>
Figure 6.26 shows the results of analyzing the vorticity data in the laminar
regime.  Shown are: a snapshot vorticity vector (upper-left), the mean
of the data (upper-right), the <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img96.gif"
 ALT="$\Psi$">
energy percentages (lower-left), 
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">(lower-right).  The analysis indicates that five eigenfunctions are necessary
to capture 95% of the energy.
<BR>
<p>
<center>
<a href=kol_laminar1.gif><img src="kol_laminar1.small.gif"></a>
<br>
Figure 6.26 Kolmogorov(vorticity, laminar): snapshot(u-l), mean(u-r),
energy %s(l-l), <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<P>
<!--------------------------------------------------->

<BR CLEAR="ALL">
<P>
In Figure 6.27, we show a different snapshot vorticity vector (upper-left)
which highlights the eyes (vortices) in the eddies, and the <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
(lower-right).  Notice that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
contains the diagonal eddy structure,
whereas <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
contains more of the vortices structure.
<BR>
<BR CLEAR="ALL">
<P>
<!--------------------------------------------------->
<p>
<center>
<a href=kol_laminar2.gif><img src="kol_laminar2.small.gif"></a>
<br>
Figure 6.27 Kolmogorov(vorticity, laminar): snapshot(u-l), mean(u-r),
energy %s(l-l), <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<!--------------------------------------------------->
Figure 6.28 shows the results of analyzing the vorticity data in the bursting
regime.  Shown are: a snapshot vorticity vector (upper-left), the mean
of the data (upper-right), the <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img96.gif"
 ALT="$\Psi$">
energy percentages (lower-left), 
and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">(lower-right).  The analysis indicates that twenty eigenfunctions are necessary
to capture at least 95% of the energy.
<BR>
<BR CLEAR="ALL">
<!--------------------------------------------------->
<p>
<center>
<a href=vort.gif><img src="vort.small.gif"></a>
<br>
Figure 6.28 Kolmogorov(vorticity, burst): snapshot(u-l), mean(u-r),
energy %s(l-l), <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>
<!--------------------------------------------------->
<P>
In Figure 6.29, we show <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
(upper-left), <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img88.gif"
 ALT="$\Psi_3$">
(upper-right),
<IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img122.gif"
 ALT="$\Psi_4$">
(lower-left), and <IMG
 WIDTH="29" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img123.gif"
 ALT="$\Psi_{11}$">
(lower-right).  Note that
<IMG
 WIDTH="29" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi11.gif"
 ALT="$\Psi_{11}$">
is an especially localized vortex structure.
<BR>
<BR CLEAR="ALL">
<P>
<!--------------------------------------------------->
<p>
<center>
<a href=vort_psi2.3.4.11.gif><img src="vort_psi2.3.4.11.small.gif"></a>
<br>
Figure 6.29 Kolmogorov(vorticity, burst): 
<img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(u-l), 
<img SRC="Psi3.gif" ALIGN="MIDDLE" BORDER="0">(u-r), 
<img SRC="Psi4.gif" ALIGN="MIDDLE" BORDER="0">(l-l), 
<img SRC="Psi11.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>
(click on image for 1278x1022 enlargement)
</center>

	 <EM>6.3.3. Conclusions</EM>

<P>
We have not shown any reconstructions of the data using the eigenfunctions.
This becomes difficult to do on paper when dealing with a sequence of
2<I>D</I> images.  However, <EM>kltool</EM> lets a user synchronize the data
vectors with the reconstructed vectors when scanning them.
This technique, as well as scanning the error vectors, allows for a
visual check of the reconstruction's accuracy to the data.

<P>
We concluded that the laminar regime is clearly a modulated traveling wave.
Its wave speed
can be calculated by observing the change in phase of a Fourier mode.  
In a coordinate frame moving with the traveling part, the oscillation
is described by a limit cycle which is spanned by the first two
eigenfunctions of the K-L decomposition for the stream function data.
We independently confirmed this by extracting a scalar ``averaged''
quantity from 
the field (summing up a norm over a 
<!-- MATH: $16 \times 16$ -->
<IMG
 WIDTH="55" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img125.gif"
 ALT="$16 \times 16$">
grid in physical space).
On the time series for that scalar quantity we performed a time-delay
embedding and calculated the fractal dimension <I>d</I><SUB><I>F</I></SUB> 
[Kostelich &amp; Swinney '89].
For both stream function
and vorticity data we found  <I>d</I><SUB><I>F</I></SUB> = 1.0. 

<P>
There exists a low-dimensional large scale dynamics that drives the
burst. We found that the unstable manifold of the laminar regime 
is very flat, concentrating the burst mainly along one dimension.
This is again supported by plots of a time-delay embedding.

<P>
Figure 6.30 shows such a 2D time-delay plot. One sees the remnants of
the basic limit cycle with faster oscillations superimposed.  Computations
of fractal dimensions suffer from too small a data set and are not very
accurate. Initial calculations give 
<!-- MATH: $d_F = 1.24$ -->
<I>d</I><SUB><I>F</I></SUB> = 1.24 and 
<!-- MATH: $d_F = 1.65$ -->
<I>d</I><SUB><I>F</I></SUB> = 1.65for stream function and vorticity data, respectively. 
Both numbers are consistent
with a K-L embedding dimension between 3 and 5 .
<BR>
<BR CLEAR="ALL">      
<!--------------------------------------------------->
<p>
<center>
<img src="tdplot.small.gif"><br>
Figure 6.30 
2D time-delay plot for an averaged scalar function
of the vorticity field. One can distinguish a laminar
dynamics following closely
the original limit cycle and a faster, oscillatory phase with motion transverse
to the limit cycle.
</center>

<P>
The biggest mystery of this analysis is the large discrepancy between 
the number of relevant eigenfunctions for stream function and vorticity 
data. From a 
dynamical systems point of view, if this system has a finite dimensional
attractor
then there exists only <EM>one</EM> dimension and all numbers for vorticity and
stream function should agree with each other. However, the Laplacian obviously
acts as a nonlinear weight function giving largest weight to the smallest scales
of the PDE simulation. Therefore the general noise level of the vorticity data
is increased up to a margin of about 5 % of the K-L energy. In order to check
the dependence of the K-L results on the resolution of the PDE simulation,
we enlarged our data set and used <EM>all</EM> Fourier modes available from
the 
<!-- MATH: $64 \times 64$ -->
<IMG
 WIDTH="55" HEIGHT="28" ALIGN="MIDDLE" BORDER="0"
 SRC="img115.gif"
 ALT="$64 \times 64$">
pseudo spectral grid. The result is that we needed even 
more K-L eigenfunctions (25 versus 20) to capture 95% of the 
energy in the
vorticity burst data. At the same time, visual inspection of the reconstruction
was satisfactory with fewer K-L modes (10 modes versus 16). These
results suggest that there are two types of dynamics going on during the
burst phase: a large scale, low dimensional one which can be described
by a structurally stable homoclinic orbit and which has very definite symmetry
properties. This dynamics is best captured by looking at the
behavior of the stream function. Riding on top of that dynamics is a small
scale dynamics characterized by an enstrophy cascade: 
enstrophy is accumulating
in the modes which are barely resolved and have the smallest scales. Better
spatial resolution leads to an accumulation of enstrophy in the new, yet smaller
scales.

<P>
For more details regarding the K-L analysis of the Kolmogorov flow,
including a more in-depth discussion of how
symmetries of the equation play an important role, see 
[Armbruster, Heiland, Kostelich &amp; Nicolaenko '92]

<P>

	 <EM>6.4. Flame Dynamics</EM>

<P>
As a final example, we present an analysis of data from 
an experiment on the dynamics of two-dimensional flames
[Gorman, el-Hamdi &amp; Robbins '92].
A schematic of the experiment is shown in Figure 6.31.
<p>
<center>
<img src="flame_exp.gif"><br>
Figure 6.31 Schematic illustration of flames experiment.
</center>
<p>
The apparatus consists of a flat, circular, porous plug burner
which burns various premixed gases.  A video camera
records the resulting flame dynamics.  
Depending on the types of gases and
their flow rate through the burner,
one can observe fascinating spatiotemporal patterns of the flames.
For example, one such pattern involves a stable rotating ring of flame
cells about a
central core of cells.  The flames can become unstable due to a competition
between thermal diffusivity and mass diffusivity resulting in some outer
cells exchanging positions with some inner cells.
<BR CLEAR="ALL">
<P>
For input to <EM>kltool</EM>, we chose a simpler regime
of flame dynamics to analyze.  The flame in this regime was a single closed 
spatial structure
which changed its shape in time.  The first few video frames
of the dynamics are shown in Figure 6.32.
<p>
<center>
<a href=flames.gif><img src="flames.small.gif"></a>
<br>
Figure 6.32 First sixteen images of flame video data (left to right, top to
bottom).
<br>(click on image to see 821x726 enlargement)
</center>

<P>
We briefly describe the preprocessing
steps involved in the analysis.  First, the video images were digitized.
Next, we subtracted the artifical reflective ring surrounding the flame.
Upon analyzing these 2<I>D</I> images, the results were difficult to interpret.
The eigenfunctions had to be mapped back into grayscale values for display
and this resulted in confusing images.
Realizing that what we actually wanted to analyze was the dynamics of just the
boundary of the flame,
we replaced the solid flame image with its boundary curve.
This not only gave more meaningful results than working with the entire 
flame image, but also significantly decreased the
computation time.
The boundary curves are 1<I>D</I> spatial vectors: 
<!-- MATH: $r(\theta),
0\leq\theta\leq2\pi$ -->
<IMG
 WIDTH="114" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img128.gif"
 ALT="$r(\theta),
0\leq\theta\leq2\pi$">,
plotted in polar coordinates.
They are shown superimposed on the flame images in Figure 6.32.
Finding a flame boundary was straightforward for this particular regime. 
A parametrized
radial sweep of a flame image yielded a unique intersection point with
the flame edge (detected by a gradient change in light intensity).

<P>
The resulting analysis by <EM>kltool</EM> is shown in Figures 6.33 and 6.34.
Figure 6.33 shows a sample snapshot in the data window (upper-left), the
mean (upper-right), the <IMG
 WIDTH="16" HEIGHT="14" ALIGN="BOTTOM" BORDER="0"
 SRC="img96.gif"
 ALT="$\Psi$">
energy percentages (lower-left), and <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">(lower-right).  The analysis indicates that only two K-L modes are
responsible for most of the dynamics.  Notice that <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">,
the
primary coherent structure, is quadrapole-shaped.
Figure 6.34 shows a different snapshot (upper-left) and a dipole-shaped 
<IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">
(lower-right).
<!--------------------------------------------->
<P>
<center>
<a href=flamefig1.gif><img src="flamefig1.small.gif"></a>
<br>
Figure 6.33 Flames: snapshot(u-l), mean(u-r), energy %s(l-l), <img SRC="Psi1.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>(click on image to see 825x819 enlargement)
</center>
<!--------------------------------------------->
<P>
<center>
<a href=flamefig2.gif><img src="flamefig2.small.gif"></a>
<br>
Figure 6.34 Flames: snapshot(u-l), mean(u-r), energy %s(l-l), <img SRC="Psi2.gif" ALIGN="MIDDLE" BORDER="0">(l-r).
<br>(click on image to see 825x819 enlargement)
</center>

<P>
In Figure 6.35, we plot the time series of coefficients for <IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi1.gif"
 ALT="$\Psi_1$">
and
<IMG
 WIDTH="23" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="Psi2.gif"
 ALT="$\Psi_2$">.

Their oscillatory behavior clearly indicates that we have a
limit cycle which is spanned by these first two K-L modes.

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt7">
	 <B>7. Future Directions</B>
</a>
<P>
There are at least two parallel directions for the future of <EM>kltool</EM>.
In one direction, we forsee added functionality.
For example, since the tool currently allows for the visualization of
only one- and 
two-dimensional spatial vectors, an obvious extension would be 
the visualization of trivariate data.
This seemingly insignificant increase by one dimension
has a significant impact on the computational cost,
as well as the visualization challenge.
One could alleviate these problems by running the tool on a dual-platform.
The lengthy computations could be performed on a more powerful processor,
then the results displayed on the graphical workstation.
Regarding the computational issue, we note that a parallel processor
would be very efficient for computing the covariance matrix.
Another extension in functionality would be to analyze vectors of 
vector values instead of vectors of scalar values.
Additional functionality might be to compute eigenfunctions which
have different optimality criteria.  Such an approach is discussed
in [Kirby '92].  An addition to the Gal&#235;rkin projection
would be to allow for numerical integration of the ODEs and display the
resulting trajectories.

<P>
The second direction for the future of <EM>kltool</EM> is to broaden 
its accessibility. 
The primary limitation at this time is its dependence on the Silicon Graphics
workstation and the Graphics Library.
One could rewrite the graphics-dependent software for other computers,
taking advantage of available hardware, and thereby offer a larger set
of platforms on which the tool could run.  However, a more likely 
option would be to
write the graphics-dependent software in a standard graphics application
language.  Although the Graphics Library itself has been ported
to other computers, the apparent standard package for graphics applications
is the X library [Nye '88].
Unfortunately, at the present time, the 3<I>D</I> graphics in the X library
does not seem to be well established.

<P>

<!--------------------------------------------------------------------------->
<dt><a NAME="chpt8">
	 <B>8. Summary</B>
</a>
<P>
An interactive graphical software package, <EM>kltool</EM>, has been
presented for the analysis of spatiotemporal data.
The tool uses the Karhunen-Lo&#232;ve (K-L) decomposition to compute
empirical eigenfunctions which optimally span a given dataset.
These eigenfunctions, <IMG
 WIDTH="21" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img94.gif"
 ALT="$\Psi_i$">,
can indicate coherent spatial structures 
within the data.  By projecting the data onto the <IMG
 WIDTH="21" HEIGHT="29" ALIGN="MIDDLE" BORDER="0"
 SRC="img94.gif"
 ALT="$\Psi_i$">,
it is
possible to gain a better understanding of their temporal dynamics
within a dynamical systems framework.
Through this analysis, one can
gain insight into the asymptotic behavior of
a given system and can get an estimate on the dimension of
an underlying attractor.

<P>
We have demonstrated <EM>kltool's</EM> effectiveness on four different example
datasets: two from PDE simulations and two from laboratory experiments.
For particular regimes of the Kuramoto-Sivashinsky (K-S) and Kolmogorov PDEs, 
we showed how certain eigenfunctions (K-L modes)
correspond to fixed points and how others span limit cycles.
We also showed how certain K-L modes are responsible for bursting behavior.
A Gal&#235;rkin projection was automatically generated for the K-S PDE
using three eigenfunctions.
The analysis of the Couette-Taylor experimental data indicated
rather complex dynamics.  It would be interesting to have more
data of this type for further analysis.
The gas flame dynamics seemed to be evolving on
a limit cycle spanned by two K-L modes.

<P>
We do not wish to leave the impression that this tool will always provide
a simple analysis of any spatiotemporal dataset.  
However, if used wisely, we feel that it can benefit many researchers
investigating nonlinear systems.
<!--------------------------------------------------------------------------->
<BR><HR>
<P>
<dt><a NAME="biblio">
	 <B>Bibliography</B>
</a>

<P>
Abraham, R.H. and Shaw, C.D. (1984),
<EM>Dynamics: The Geometry of Behavior</EM>, 
Ariel Press, Santa Cruz.

<P>
Armbruster, D., Guckenheimer, J. and Holmes, P. (1989),
Kuramoto-Sivashinsky dynamics on the center-unstable manifold,
SIAM J. Appl. Math. 49,
676-691.

<P>
Armbruster, D. and Heiland, R. (1992),
<EM>kltool:</EM> User's Manual.
Dept. Mathematics, Arizona State University, Tempe.

<P>
Armbruster, D., Heiland, R., Kostelich, E.J. and Nicolaenko, B. (1991),
Phase space analysis of bursting behavior in Kolmogorov flow,
to appear in Physica D.

<P>
Aubry, N., Holmes, P., Lumley, J.L. and Stone, E. (1988),
The dynamics of coherent structures in the wall region of a turbulent
boundary layer,
J. Fluid Mech. 192,
115-173.

<P>
Barnsley, M.F. (1988),
<EM>Fractals Everywhere</EM>, 
Academic Press, London,
172-199.

<P>
Broomhead, D.S. and King, G.P. (1986),
Extracting qualitative dynamics from experimental data, 
Physica 20D,
217-236.

<P>
Chorin, A.J. and Marsden, J.E. (1990),
<EM>A Mathematical Introduction to Fluid Mechanics</EM>, 
Springer-Verlag, New York,
32-46.

<P>
Crawford, J.D. (1991),
Introduction to bifurcation theory, 
Rev. Modern Physics 63,
991-1037.

<P>
DiPrima, R.C. and Swinney, H.L. (1981),
Instabilities and transition in flow between concentric rotating cylinders,
in: Swinney, H.L. and Gollub, J.P., eds.,
<EM>Hydrodynamic Instabilities and the Transition to Turbulence</EM>,
Springer-Verlag, New York,
139-180.

<P>
Eckmann, J.P. (1981),
Roads to turbulence in dissipative dynamical systems, 
Rev. Modern Physics 53,
643-654.

<P>
Farmer, J.D., Ott, E. and Yorke, J.A. (1983),
The dimension of chaotic attractors, 
Physica 7D,
153-180.

<P>
Fukunaga, K. (1990),
<EM>Introduction to Statistical Pattern Recognition</EM>, 
Academic Press, San Diego.

<P>
Gibson, J.F., Farmer, J.D., Casdagli, M. and Eubank, S. (1992),
An analytic approach to practical state space reconstruction, 
Santa Fe Institute report #92-04-021.

<P>
Gleick, J (1987),
<EM>Chaos: Making a New Science</EM>,
Viking Press, New York.

<P>
Gonzalez, R.C. and Wintz, P. (1987),
<EM>Digital Image Processing</EM>, 2nd Edition,
Addison-Wesley, Reading, MA,
122-130.

<P>
Gorman, M., el-Hamdi, M. and Robbins, K.A. (1992),
Spatiotemporal chaotic dynamics of premixed flames,
in: Vohra, S., Spano, M., Shlesinger, M., Pecora, L. and Ditto, W., eds.,
<EM>Proceedings of the 1st Experimental Chaos Conference</EM>,
World Scientific, Singapore,
403-416.

<P>
Guckenheimer, J. and Holmes, P. (1983),
<EM>Nonlinear Oscillations, Dynamical Systems, and Bifurcations
of Vector Fields</EM>, 
Springer-Verlag, New York.

<P>
Guckenheimer, J. and Kim, S. (1990),
<EM>kaos: Dynamical Systems Toolkit with Interactive Graphics Interface</EM> 
Mathematics Dept., Cornell University.

<P>
Hirsch, M.W. and Smale, S. (1974),
<EM>Differential Equations, Dynamical Systems, and Linear Algebra</EM>,
Academic Press, Orlando.

<P>
Hyman, J.M., Nicolaenko, B. and  Zaleski, S. (1986),
Order and complexity in the Kuramoto-Sivashinsky model of weakly
turbulent interfaces,
Physica 23D,
265-292.

<P>
Jolly, M.S, Kevrekidis, I.G. and Titi, E.S. (1990),
Approximate inertial manifolds for the Kuramoto-Sivashinsky equation:
analysis and computations,
Physica 44D,
38-60.

<P>
Kevrekidis, I.G., Nicolaenko, B. and Scovel, J.C. (1990),
Back in the saddle again: A computer assisted study of the Kuramoto-Sivashinsky
equation,
SIAM J. Appl. Math. 50,
760-790.

<P>
Kirby, M. (1992),
Minimal dynamical systems from PDEs using Sobolev eigenfunctions,
to appear in Physica D.

<P>
Kirby, M. and Armbruster, D. (1991),
Reconstructing phase space for PDE simulations,
to appear in ZAMP.

<P>
Ko&#231;ak, H. (1986),
<EM>Differential and Difference Equations through Computer Experiments</EM>,
Springer-Verlag, New York.

<P>
Kostelich, E.J. (1992)
Problems in estimating dynamics from data, 
to appear in Physica D.

<P>
Kostelich, E.J. and Swinney, H.L. (1989),
Practical considerations in estimating dimension from time series data,
Physica Scripta 40,
436-441.

<P>
Kostelich, E.J. and Yorke, J.A. (1990),
Noise reduction: Finding the simplest dynamical system consistent with
the data,
Physica 41D,
183-196.

<P>
Libchaber, A., Fauve, S. and Laroche, C. (1983),
Two-parameter study of the routes to chaos,
Physica 7D,
73-84.

<P>
Lorenz, E.N. (1963),
Deterministic non-periodic flow, 
J. Atmos. Sci. 20,
130-141.

<P>
Lumley, J.L. (1967),
Atmospheric Turbulence and Radio Wave Propagation, Yaglom, A.M. and 
Tatarski, V.I., eds., Nauka, Moscow,
166-.

<P>
Mees, A.I., Rapp, P.E. and Jennings, L.S. (1987),
Singular-value decomposition and embedding dimension, 
Physical Rev. A 36,
340-346.

<P>
Newell, A.C. and Moloney, J.V. (1992),
<EM>Nonlinear Optics</EM>, 
Addison-Wesley, Redwood City, CA.

<P>
Newell, A.C., Rand, D.A. and Russell, D. (1988),
Turbulent transport and the random occurrence of coherent events,
Physica 33D,
281-303.

<P>
Nicolaenko, B. and She, Z.S. (1991),
Symmetry breaking homoclinic chaos and vorticity bursts 
in periodic Navier-Stokes flows, 
Eur. J. Mech., B/Fluids 10,
67-74.

<P>
Nye, A. (1988),
<EM>Xlib Programming Manual</EM>,
O'Reilly &amp; Associates, Inc., Sebastopol, CA.

<P>
Rodriguez, J.D. and Sirovich, L. (1990),
Low-dimensional dynamics for the complex Ginzburg-Landau equation, 
Physica 43D,
77-86.

<P>
Roux, J.-C. (1983),
Experimental studies of bifurcations leading to chaos in
the Belousof-Zhabotinsky reaction, 
Physica 7D,
57-68.

<P>
Ruelle, D. (1989),
<EM>Chaotic Evolution and Strange Attractors</EM>, 
Cambridge University Press, Cambridge, England,
23-27.

<P>
Ruelle, D. (1991),
<EM>Chance and Chaos</EM>, 
Princeton University Press, Princeton, NJ,
51-65.

<P>
Sauer, T., Yorke, J.A. and Casdagli, M. (1991),
Embedology,
J. Statist. Phys. 65,
579-616.

<P>
She, Z.S. (1987),
Metastability and vortex pairing in Kolmogorov flow, 
Phys. Lett. A 124,
161-164.

<P>
She, Z.S. and Nicolaenko, B. (1990),
Temporal intermittancy and turbulence production in the Kolmogorov flow, 
in: Moffatt, H.K., ed.,
<EM>Topological Fluid Mechanics</EM>,
Cambridge University Press, Cambridge, England.

<P>
Shneiderman, B. (1980),
<EM>Software Psychology</EM>, 
Little, Brown and Co., Boston,
216-266.

<P>
Sirovich, L. (1987a),
Turbulence and the dynamics of coherent structures, 
Part I: Coherent Structures,
Quarterly of Appl. Math. XLV,
561-571.

<P>
Sirovich, L. (1987b),
Turbulence and the dynamics of coherent structures, 
Part III: Dynamics and Scaling,
Quarterly of Appl. Math. XLV,
583-590.

<P>
Sirovich, L. and Kirby, M. (1987),
Low-dimensional procedure for the characterization of human faces,
J. Opt. Soc. Am. A 4,
519-524.

<P>
Sparrow, C. (1982),
<EM>The Lorenz Equations</EM>, 
Springer-Verlag, New York.

<P>
Stewart, I. and Golubitsky, M. (1992),
<EM>Fearful Symmetry</EM>, 
Blackwell, Cambridge, MA,
104-118.

<P>
Stone, E. (1992),
Bifurcation structures of minimal ODEs from weighted Sobolev projections 
of PDEs, in:
<EM>Proceedings of Conference on Bifurcations and Symmetries</EM>,
SIAM AMS Summer Workshop,
Ft. Collins, CO.

<P>
Strang, G. (1976),
<EM>Linear Algebra and its Applications</EM>, 
Academic Press, New York.

<P>
Strang, G. (1986),
<EM>Introduction to Applied Mathematics</EM>, 
Wellesley-Cambridge Press, Wellesley, MA.

<P>
Sugihara, G. and May, R.M. (1990),
Nonlinear forecasting as a way of distinquishing chaos from
measurement error in time series, 
Nature 344,
734-741.

<P>
Swinney, H.L. (1984),
Observations of complex dynamics and chaos,
in: Cohen, E.G.D., ed.,
<EM>Fundamental Problems in Statistical Mechanics VI</EM>,
North-Holland, Amsterdam.

<P>
Takens, F. (1981),
Detecting strange attractors in turbulence,
in: Rand, D.A. and Young, L.-S., eds.,
<EM>Dynamical Systems and Turbulence</EM>,
Lecture Notes in Math. 898,
Springer-Verlag, New York,
366-381.

<P>
Waltman, P. (1986),
<EM>A Second Course in Elementary Differential Equations</EM>, 
Academic Press, New York.

<P>
Wiggins, S. (1988),
<EM>Global Bifurcations and Chaos</EM>, 
Springer-Verlag, New York.

<P>
Wiggins, S. (1990),
<EM>Introduction to Applied Nonlinear Dynamical Systems and Chaos</EM>, 
Springer-Verlag, New York.

<P>
Yorke, J. (1990),
<EM>Dynamics: A Program for IBM PC Clones</EM>, 
Depart. Mathematics,
University of Maryland, College Park.

<!--------------------------------------------------------------------------->
<BR><HR>
<P>
<dt><a NAME="appdx">
<DIV ALIGN="CENTER"><B>
Appendix A</B></DIV>
<DIV ALIGN="CENTER"><B>
Gal&#235;rkin Projection for Kuramoto-Sivashinsky PDE</B></DIV>
</a>

<P>

<P>
This appendix accompanies the analysis in Section 6.1.1.
The input file specifying the K-S PDE, which will be
parsed by <EM>kltool</EM>,
is the following:

<P>
<PRE>
u(t,x) 
# This file provides the Kuramoto-Sivashinsky PDE
# in the desired syntax for parsing by kltool.
# The first line of the file specifies the dependent function
# and independent variables.
#
# d(u,t) = 
 -4*d(u,x,4) - alpha * ( d(u,x,2) + 0.5*d(u,x)^2 )
</PRE>
<P>
The following is the resulting Gal&#235;rkin projection,
a system of ODEs, obtained from the first three K-L modes:

<P>
<PRE><TT>
a1 * 4.77502174e-01 =  -  4  * <BR> ( a1 * 7.64229734e+00  + a2 * -1.30090583e-01  + a3 * 8.35663964e-02  )  <BR> -  alpha  *  ( <BR> ( a1 * -1.90954685e+00  + a2 * 2.70510994e-02  + a3 * -1.68602983e-02  )  <BR> +  0.5  * <BR> ( a1*a1 * 1.46427749e-02 + a1*a2 * -3.82984342e-03 + a1*a3 * -4.23164926e-02 <BR> + a2*a1 * -3.82984342e-03 + a2*a2 * -2.42889080e-04 + a2*a3 * -2.24138759e-01 <BR> + a3*a1 * -4.23164926e-02 + a3*a2 * -2.24138759e-01 + a3*a3 * 9.23985816e-03 ))
<BR>
<BR>
a2 * 5.21931895e-01 =  -  4  * <BR> ( a1 * -1.30090583e-01  + a2 * 5.43366793e-01  + a3 * 3.00340148e-04  )  <BR> -  alpha  *  ( <BR> ( a1 * 2.70510994e-02  + a2 * -5.24061964e-01  + a3 * 1.73820964e-04  )  <BR> +  0.5  * <BR> ( a1*a1 * -9.71341170e-03 + a1*a2 * 1.04331524e-02 + a1*a3 * 5.34356186e-01 <BR> + a2*a1 * 1.04331524e-02 + a2*a2 * -2.16584341e-04 + a2*a3 * -8.41680678e-03 <BR> + a3*a1 * 5.34356186e-01 + a3*a2 * -8.41680678e-03 + a3*a3 * 1.24558107e-02 )) <BR>
<BR>
a3 * 5.26295447e-01 =  -  4  * <BR> ( a1 * 8.35663964e-02  + a2 * 3.00340148e-04  + a3 * 6.41147474e-01  )  <BR> -  alpha  *  ( <BR> ( a1 * -1.68602983e-02  + a2 * 1.73820964e-04  + a3 * -5.33260817e-01  )  <BR> +  0.5  * <BR> ( a1*a1 * 3.11884903e-02 + a1*a2 * 5.28333621e-01 + a1*a3 * -9.22197842e-03 <BR> + a2*a1 * 5.28333621e-01 + a2*a2 * -3.38895660e-02 + a2*a3 * 3.73078284e-03 <BR> + a3*a1 * -9.22197842e-03 + a3*a2 * 3.73078284e-03 + a3*a3 * -1.90138218e-02 ))
<BR>
</TT></PRE>
<P>
These equations represent 
<!-- MATH: $d{\bf a}/dt$ -->
<IMG
 WIDTH="43" HEIGHT="31" ALIGN="MIDDLE" BORDER="0"
 SRC="img129.gif"
 ALT="$d{\bf a}/dt$">.
The user would need to divide
each equation by the constant on the left-hand side before inserting the 
system into a numerical ODE solver.
<BR><HR>
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS">&#160;</A>
<UL>
<LI><A NAME="tex2html7"
 HREF="node1.html">About this document ... </A>
</UL>
<HR>

</BODY>
</HTML>
